# Sign-language-recognition-

## About
Sign language recognition leveraging LSTM (Long Short-Term Memory) networks and MediaPipe entails the fusion of deep learning methodologies with computer vision techniques to interpret gestures in sign language, captured through video input. Here's how each component contributes to the process:Firstly, LSTM networks are employed for their prowess in capturing sequential dependencies. Particularly adept at tasks involving temporal sequences, LSTM networks excel in analyzing patterns over time, making them well-suited for sign language recognition. In this context, LSTM models can scrutinize the sequence of hand gestures captured from video frames, discerning nuanced movements and their temporal relationships.Complementing LSTM's temporal analysis capabilities is MediaPipe, an open-source framework developed by Google. MediaPipe offers a suite of tools for multimodal applied ML pipelines, including hand tracking, which is crucial for sign language recognition. By employing MediaPipe's hand tracking module, the system can accurately detect and track hand movements within the video frames, providing the necessary input data for the LSTM model.The workflow for building such a system typically involves several key steps. 

## Features
<!--List the features of the project as shown below-->
- Detection of Hand Keypoints
- A framework based application for deployment purpose.
- High scalability.
- Less time complexity.

## Requirements
<!--List the requirements of the project as shown below-->
* Operating System: Requires a 64-bit OS (Windows 10 or Ubuntu) for compatibility with deep learning frameworks.
* Development Environment: Python 3.6 or later is necessary for coding the sign language detection system.
* Deep Learning Frameworks: TensorFlow for model training, MediaPipe for hand gesture recognition.
* Image Processing Libraries: OpenCV is essential for efficient image processing and real-time hand gesture recognition.
* Version Control: Implementation of Git for collaborative development and effective code management.
* IDE: Use of VSCode as the Integrated Development Environment for coding, debugging, and version control integration.

## System Architecture
![image](https://github.com/VigneshKumar1009/Sign-language-recognition-/assets/113573894/2534f638-0b8c-4cd5-b0a6-df51e15e4a9a)

## Output

### Output1 - Keypoints Detection
![image](https://github.com/VigneshKumar1009/Sign-language-recognition-/assets/113573894/3198789d-c5b4-4373-b5bb-1ca54152d8f0)

### Output2 - Detection of Gestures
![image](https://github.com/VigneshKumar1009/Sign-language-recognition-/assets/113573894/e5a2a207-a4e8-411f-a59e-ab3539cad691)

## Results and Impact
In summary, Sign language recognition leveraging LSTM and MediaPipe delivers significant outcomes and societal impacts. Firstly, it markedly enhances accessibility for the deaf and hard-of-hearing community by providing an effective means to interpret sign language gestures. This technology enables seamless communication between sign language users and non-signers, fostering inclusivity in various domains such as education, workplaces, and public services. Real-time recognition capabilities facilitate immediate interaction, breaking down communication barriers and empowering individuals to engage naturally with digital devices and platforms. Moreover, in educational settings, these systems support efficient learning and teaching of sign language, offering feedback and assistance to learners as they develop their signing skills. As assistive technology, integration with smartphones, tablets, and wearables enables independent access to digital content, promoting autonomy and inclusion. Beyond individual applications, the data collected from sign language recognition systems fuels research in linguistics, gesture analysis, and accessibility technology, driving continuous innovation in this field. Overall, sign language recognition using LSTM and MediaPipe not only enhances accessibility and communication but also fosters innovation and inclusivity in society.

## Articles published
1. H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer. Multiscale vision trans-formers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 4 2021.
2. M. D. Coster, M. V. Herreweghe, and J. Dambre. Isolated sign recognition from rgb video using pose flowand self-attention. CVPR, 2021.
3. S. Albanie, G. Varol, L. Momeni, T. Afouras, J. S. Chung, N. Fox, and A. Zisserman. Bsl-1k: Scalingup co-articulated sign language recognition using mouthing cues. European conference on computer vision,2020.
4. Y. C. Bilge, N. Ikizler, and R. Cinbis. Zero-shot sign language recognition: Can textual data uncover signlanguages? BMVC, 2019.
5. N. C. Camgoz, S. Hadfield, O. Koller, H. Ney, and R. Bowden. Neural sign language translation. CVPR,2018.
6. N. C. Camgoz, S. Hadfield, O. Koller, and R. Bowden. Subunets: End-to-end hand shape and continuoussign language recognition. ICCV, 2017.
7. N. C. Camgoz, S. Hadfield, and O. Koller. Using convolutional 3d neural networks foruser-independentcontinuous gesture recognition. 23rd International Conference on Pattern Recognition, 2016.
8. E. Antonakos, A. Roussos, and S. Zafeiriou. A survey on mouth modeling and analysis for sign languagere-cognition. IEEE International Conference, 2015. 
9. H. Cooper, N. Pugeault, and R. Bowden. Reading the signs: A video based sign dictionary. IEEE Interna-tional Conference on Computer Vision Workshops., 2011.
10. P. Buehler, M. Everingham, and A. Zisserman. Learning sign language by watching tv (using weakly alignedsubtitles). Computer Vision and Pattern Recognition, 2009.















